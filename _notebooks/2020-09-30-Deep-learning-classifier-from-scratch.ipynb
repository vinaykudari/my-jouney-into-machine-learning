{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"American sign language classifier from scratch\"\n",
    "\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [classifier, scratch, vision]\n",
    "- hide: false\n",
    "- search_exclude: true\n",
    "- metadata_key1: metadata_value1\n",
    "- metadata_key2: metadata_value2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import pandas as pd\n",
    "from fastbook import plot_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3676, 785), (919, 785))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/code/datasets/mnist_sign/sign_mnist_train.csv')\n",
    "classes = [3, 1, 23, 22]\n",
    "df = df[df.label.isin(classes)]\n",
    "train_df = df.sample(frac=0.8, random_state=100)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(df):\n",
    "    return tensor(df.iloc[:, 0].values).unsqueeze(1)\n",
    "\n",
    "def get_image_tensors(df):\n",
    "    return torch.stack([tensor(image_array)/255. for image_array in df.iloc[:, 1:].values])\n",
    "\n",
    "def get_dataset(df):\n",
    "    return zip(get_labels(df), get_image_tensors(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3])\n",
      "tensor([22])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAMV0lEQVR4nO1baW8bRxJ9c58cjilHCi0Fjpk4FqIPyef8n/zpAIERJ5KZkBKP4dzXfjCqt9nqOSh7sYuFChiI5Bzd/br61avqkdK2LZ7t36b+tzvwv2bPgAj2DIhgz4AI9gyIYHrfyV9//bXVNA2+78MwDLiuC13XYVkWNE2DYRjQNA26rkNVVWiaBlVVj74risJ+VxQFqqo+OhRFQVVVKMsSpmnCMIyj6xVFYX1SFOXoO/87AIhRs2mao+90/pdffnn8kCFAqHGxE/RZ1rGuZ/SZqqooyxJJkqBpGiiKAl3XoWna4PPHtH+KtOgFhDpFf7tmmgYtfhcPup88hwau6zrW6zV+//13zOdznJ+fYzqdwrKso/7wA+sCecwk9Vkvh/QNkBrv+q2rs7Jz5CFxHCPLMhRFgbquB5831sT+9dkoQLrWfZcHyLxF5AKxjaIosN1uEUUR4jhGURRo25Z5hWzpioMW2+wCpg+UQUC63F9Vj28dIjvZDNH6btuWkWpd151rfqynDF3Xd76XQ/jIcYqnyMATQaRnA0BRFMiyDHEco6qqozZPAWLMoIcIdhCQvoFSw3zHeQC6IhR/XdM0zDuqqgIABgh//VMA4H8bG2lGh13xuxg1eK+hKNLnSbquw3VdxhlJkiBNUyiKAs/zYBjGqDXfB4gMjKFnjQZkzKzLOth1D4VzACjLkgkoXvCNNb4tnoSBT2CcsuQGl8xY/uA7wd8v45C2bZmHbLdbZFmGuq6hqiosy4LneQysIQ+RnedBUVX1ywkzGT+IHZF5xhDvEKBt2zKFWpYlE398+KZnijPfB4h43ylc0guIjCN4ldlHluJ9PO+YpglN01DXNeI4xmq1QlEUMAwDhmHAsixpLiNqki5AeBOBFHObkwDpiipjuKXLg4g7FOVTQpemKaIoQtM0cByHJXYi8PxsD4HBe4UMzD4v6QWETOSNIW4RXV4ExHVdFEWBNE2x2Wxwd3cHy7Lw8uVLTCYTmKZ5tLxETxwy/hpa8pQKDNkoDunzCvH6oUMETtd1mKYJy7JgWRYURTniE1k7okrmTVwSMh55MiB9KrWPNPsAJCAMwwAAuK4L3/dh2zZs24aiKEiSBI7jHIXeLk8R3V/TNLRty4DhPY2uf/KS6ct0ZQOXmSzC8J7i+z6+/vpr5iV1XWO5XML3fbiui8lkAsdxjmZeFvlE8UXXUH1F7NOTAOG5oMv1x5AsdYCPWuTGs9kMP/zwA6uL/Pnnn/jjjz8QhiEmkwnevHkDx3GO+tU0DfMennD52edBk3nMkwDpWgJd52TXip7BX8dzwcPDA/b7PQ6HA8qyxH6/R57nOD8/x8uXL1HXNZqmQVEUqKoKQRAwzpENtIt3Pivs8gPkB9HlOUPFJFFsUe21rmt8+PABv/32G3zfh+d5iKIIZVni6uoKVVWhKAqUZYkoipDn+RHn8H0V+UEM1UOgjJLu4mD482MiCwku27aZzqjrmg2SMl0KxU3TsLbTNMV2u2WD2+/3SJIEFxcXvdxF1/PSXVxOTwJENvPAcarfp08otDqOg+l0ymanrmtkWYY8z1EUBfI8R57nqKoKcRwjCAJ4nseUrGma0HUdm80G+/0ei8VCOrAuku2S/icBIjbUFVFkoVVVVZa1mqYJAMiyjNU/0jRFHMfYbDb4+++/kWUZTNM8qosYhsEAyvMciqJgtVphs9kgiiKkacq2RHjjl4noHbJldRIgokeIn4HHWTHxA82qZVlomgZJkqCqKuR5jjRNkSQJVqsVbm9v2eDKskRRFFAUhQEURREb1MePH3F/f4/tdos0Tdly5MEQTQTjyWGXH7BIjvzBg6RpGhzHga7rsG2beQiFSlrTRVGgaRqW7dJBUSTPc2RZBgBHnkCc89dff0FVVVxfX7PtCr6PIjDU7hdZMjL9IeoQuo4AoayVVGlVVYwsFUVBlmUMmMPhgDRNkaYp6rpGXdfI85xtS6iqiiAIYNs2iqJAURT48OED4jjG5eUlZrNZZ/iVjeXJgAyFVd7IE3zfRxiGAMB0w3a7ZZ5A0YSWjaZpCMOQLam6rtkyoWp80zS4uLjAZDJB27YwTRNJkgD45D1VVbGSI/B42QyF5tGA8F7QBRJ/rW3bcF0Xs9mMDaooCtzf37M0n8CgCESAJEmCtm0ZIARekiTI85xV0Nq2hWEYjGip2kaapg8M2feTABEfJHqI+J0XWuTym80GHz9+xHa7xXK5RBRF2G63cF0XnufBsiy4rgvXdeE4DtMmwKdaa57nKMvyiGSpP0TY1K4svIrhd0i7jAZEfKBsOfGAFEWBOI6x2+2wWq2wXC7x/v17PDw8YLlcIgxDnJ2d4ZtvvsG3334L27aZFKdlAgBVVaGqKhbGKYQTN9HRJ7Z4gIZqrCdxiIiu6CkAWCJVFAV2ux02mw3++ecfrFYrrFYrrNdr3N3dYbfb4eHhAb7v4+3bt0eDr6qKJXv8Rju/vTGZTJiH8YMlo0RP9JTPDrt8ZBkCTATkcDhgv9/j4eEBm80Gm80G9/f3WK/X2O/32G63+O67747CMQk3Apl/x4Rvy3VdBEHAyJQ/z2e3vH122JURUp/HNE1zNDO0xn3fx+FwgOM4rDzoOA5evHjBxFiSJDgcDojjmG1YUQjnK2kUdqfTKebzOduyED2X+sP/NlQcGgVIF1+IXsPXIwgQymMoMyXlqiifEj56M4kiC5/bmKbJogdltfyWp+M4CMPwkXTn2+f7JysUPRmQLpPtefCzQIBQphsEAaIowmw2Y1pitVrh/fv3WK/XSJIEuq7jxYsXrEBEZAuARZy6ruF5HsIwhGmaj4hSXML0W9dSGg3IkMkySHGG+EKybdvwPA+e56FpGuR5jv1+j/V6zTQKeUQQBAiCgJEqH33atj3a4eMHLypWvj9fxEPor4xPZBmuuFZVVWVaYzqdIs9zxhX7/R5FUSCKIpimiaurK+YRVHgW6yW07IIgwHQ6Pdqy6Jsw8qKh8Dx6G0J2TiRWmauSPiEPcRwHnucxgUWaxXEcdlA4NU0TcRwDAAOFvIyEHM8foqfK+vvZVfeu4o8MpLqupW8CibKaRBUNbjqdIgxD9qIdlQvatkUcx6jrmkWcn376Ca9fv8bl5eXRlqdY9xBB+o+/HyKzpmnQNA0rCItrl/caAoU8h9cVhmEcSXjKknVdx6tXr7BYLBAEwZH24Mmyqzo2pmrWC4joXl0hmBogIMRnUEpfliVbx4ZhMI3BvydCGS4BQmWBd+/e4fLyEovFgu3j8G2TB3blMmMIdRAQHghZ7aOL0fmO0F/yHrqel+b8FgEtNZ5M67pGGIaYz+cIwxCu63b2Uxy87O2hL57LyDapyEi2k6qkv3VdQ1E+ZayTyYS5O68jKI9p2xZJkrCKme/7uLy8xNu3bxGGISNSfmAENl8moDHw4fiL65AuTuH3TmmJEMnSrJN6NU2TkS8/OLoHAJPoFG0mk4lUiHVtLYg7dTLvORmQrryF9xLqFMlxVVUfESsA9goVnadCclVVyLKM8QYBmOc56rrG9fU1FosFXr9+Dc/zHoXZLuKkdvha6hfRIX2cQZ/5cMrHeh4QihLiuyNUaKbraNkAgGmamM1mePXqFSaTyVF1fSil4EEZ6x2DgJB1lRHJSJ7rus7Sd3J/AoV/H5VP4kjC0/2GYUDXddzc3OD777/H9fU1U7CyEC6CIA6cNtaBcQneyRwicznxXzl4MHhtQpxCB/EFDxqp2fPzc1xdXbG3isQJEUEQz/Hnxy4XYKRSpYeK+zOapsGyLJydnaGqKrYRvdvtkGUZ24w6HA6Iogj39/fsO72bSrNG2uTm5gY///wz3rx5g/l8flQRo3b7wuuQIv1i9RDZb7zaJOO3Geig0Ev/+kH6gg+BmqbBtm1WZz07O4Pv+53ti4MTwZEtjTHy/aSNKt5DdF3H2dkZi/u6rmM6naJtWyyXS+x2O9ze3rKX+slLCBzR5vM53r17hx9//BGLxYJlteJgRYBENS3jCf6a/0g9hNcT/LokYqT3UMlbyCt4bhHNdV189dVXCIIAjuN05k1jvovFZdnnLnsSqVqWxcgP+FTJosam0ylubm5we3uL3W6Hpmlwd3eHLMukapEGfXFxgZubG5yfn/dm1DLjQ7+oO8ZmuWRP8hC+DCA2SLzCJ21dXsEbJXt0z+fYKbrj0b2nIvj/bs//yCzYMyCCPQMi2DMggj0DItgzIIL9C9b4/Uy+aVhUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for label, image_tensor in list(dataset)[:2]:\n",
    "    print(label)\n",
    "    show_image(torch.reshape(image_tensor, (28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "??show_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_weights(rows, cols):\n",
    "    return torch.randn(rows, cols).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 4]), torch.Size([1, 4]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = get_random_weights(28*28, len(classes))\n",
    "bias = get_random_weights(1, len(classes))\n",
    "weights.shape, bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3676, 784]), torch.Size([784, 4]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensors = get_image_tensors(train_df)\n",
    "image_tensors.shape, weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(image_tensors, weights, bias):\n",
    "    return image_tensors@weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3676, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = get_predictions(image_tensors, weights, bias)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check initial accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2557)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image_tensors, test_labels = get_image_tensors(test_df), get_labels(test_df)\n",
    "test_preds = get_predictions(test_image_tensors, weights, bias)\n",
    "pred_classes = torch.argmax(test_preds, dim=1)\n",
    "\n",
    "# This will select the class value based on index of pred_classes\n",
    "preds = tensor(classes).gather(0, pred_classes)\n",
    "(preds == test_labels).float().mean() \n",
    "\n",
    "# Random weights are ~26% accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 3, 0, 3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = get_labels(train_df)\n",
    "# get each label class\n",
    "target_classes = tensor([classes.index(c) for c in labels.view(-1)])\n",
    "target_classes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.5480e-16, 1.2497e-05, 8.2206e-08, 9.9999e-01], grad_fn=<SelectBackward>),\n",
       " tensor([1.5480e-16, 1.2497e-05, 8.2206e-08, 9.9999e-01], grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax function turns the predictions into probabilities and amplifies the predictions \n",
    "def softmax(predictions):\n",
    "    return torch.exp(predictions)/torch.exp(predictions).sum(dim=1, keepdim=True)\n",
    "# pytorch equivalent\n",
    "activations_py = torch.softmax(predictions, dim=1)\n",
    "activations = softmax(predictions)\n",
    "\n",
    "activations[0], activations_py[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2702, grad_fn=<MeanBackward0>),\n",
       " tensor(0.2702, grad_fn=<NegBackward>))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean of corresponding activations based on the target class\n",
    "loss = activations[range(len(target_classes)), target_classes].mean()\n",
    "# pytorch equivalent \n",
    "loss_py = -F.nll_loss(activations, target_classes)\n",
    "\n",
    "loss, loss_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as softmax function turns predictions into probabilities, each activation is bounded between (0, 1) hence our model considers 0.900 and 0.999 as the same but the second prediction is 100 times more confident hence we use log to amplify the domain to (-inf, inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.5480e-16, 1.2497e-05, 8.2206e-08, 9.9999e-01], grad_fn=<SelectBackward>),\n",
       " tensor([-3.6404e+01, -1.1290e+01, -1.6314e+01, -1.2517e-05], grad_fn=<LogBackward>))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations[0], torch.log(activations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(18.2121, grad_fn=<NegBackward>),\n",
       " tensor(18.2121, grad_fn=<NllLossBackward>))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax + log + negative_loss_likelyhood = cross_entropy_loss: this can be used for classification problems\n",
    "def cross_entropy_loss(predictions, target_classes): \n",
    "    activations = softmax(predictions)\n",
    "    activations_log = torch.log(activations)\n",
    "    loss = activations_log[range(len(target_classes)), target_classes].mean()\n",
    "    return -loss\n",
    "# pytorch equivalent\n",
    "cross_entropy_loss_py = F.cross_entropy(predictions, target_classes)\n",
    "\n",
    "cross_entropy_loss(predictions, target_classes), cross_entropy_loss_py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimise weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
