{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"American sign language classifier from scratch [99.63% accurate]\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [classifier, scratch, vision]\n",
    "- image: images/asl.jpg\n",
    "- hide: false\n",
    "- author: Vinay Kudari\n",
    "- search_exclude: true\n",
    "- metadata_key1: deeplearning\n",
    "- metadata_key2: image-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning based image classifier written from scratch using PyTorch and some of the Fastai functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![American Sign Language Classifier](images/asl.jpg \"Credits: Dictionary.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: [Kaggle](https://www.kaggle.com/datamunge/sign-language-mnist)\n",
    "\n",
    "Benchmarks:\n",
    "* ~99.63% accuracy with one layer of parameters and no pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import pandas as pd\n",
    "from os import path\n",
    "torch.cuda.set_device(0)\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21964, 785), (5491, 785), (7172, 785))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if path.exists('/storage'):\n",
    "    # Paperspace\n",
    "    train_df = pd.read_csv('/storage/data/asl/sign_mnist_train.csv')\n",
    "    test_df = pd.read_csv('/storage/data/asl/sign_mnist_test.csv')\n",
    "elif path.exists('/Users/vinay/Datasets/'):\n",
    "    # Local\n",
    "    train_df = pd.read_csv('/Users/vinay/Datasets/asl/sign_mnist_train.csv')\n",
    "    test_df = pd.read_csv('/Users/vinay/Datasets/asl/sign_mnist_test.csv')\n",
    "else:\n",
    "    # GCP\n",
    "    train_df = pd.read_csv('/home/jupyter/datasets/asl/sign_mnist_train.csv')\n",
    "    test_df = pd.read_csv('/home/jupyter/datasets/asl/sign_mnist_test.csv') \n",
    "\n",
    "# Randomly select 20% of training data as validation data\n",
    "valid_df = train_df.sample(frac=0.2)\n",
    "train_df = train_df.drop(valid_df.index)\n",
    "\n",
    "train_df.shape, valid_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is arranged such that each row in the dataframe consists of an image and its corresponding label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all the labels\n",
    "def get_labels(df):\n",
    "    return tensor(df.iloc[:, 0].values).to('cuda')\n",
    "\n",
    "# Select all the pixel intensities and convert into a 1-D tensor of values between 0 to 1\n",
    "def get_image_tensors(df):\n",
    "    return torch.stack([tensor(image_array)/255. for image_array in df.iloc[:, 1:].values]).to('cuda')\n",
    "\n",
    "def get_dataset(df):\n",
    "    return zip(get_image_tensors(df), get_labels(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n",
      "tensor(6)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAMV0lEQVR4nO1baW8bRxJ9c58cjilHCi0Fjpk4FqIPyef8n/zpAIERJ5KZkBKP4dzXfjCqt9nqOSh7sYuFChiI5Bzd/br61avqkdK2LZ7t36b+tzvwv2bPgAj2DIhgz4AI9gyIYHrfyV9//bXVNA2+78MwDLiuC13XYVkWNE2DYRjQNA26rkNVVWiaBlVVj74risJ+VxQFqqo+OhRFQVVVKMsSpmnCMIyj6xVFYX1SFOXoO/87AIhRs2mao+90/pdffnn8kCFAqHGxE/RZ1rGuZ/SZqqooyxJJkqBpGiiKAl3XoWna4PPHtH+KtOgFhDpFf7tmmgYtfhcPup88hwau6zrW6zV+//13zOdznJ+fYzqdwrKso/7wA+sCecwk9Vkvh/QNkBrv+q2rs7Jz5CFxHCPLMhRFgbquB5831sT+9dkoQLrWfZcHyLxF5AKxjaIosN1uEUUR4jhGURRo25Z5hWzpioMW2+wCpg+UQUC63F9Vj28dIjvZDNH6btuWkWpd151rfqynDF3Xd76XQ/jIcYqnyMATQaRnA0BRFMiyDHEco6qqozZPAWLMoIcIdhCQvoFSw3zHeQC6IhR/XdM0zDuqqgIABgh//VMA4H8bG2lGh13xuxg1eK+hKNLnSbquw3VdxhlJkiBNUyiKAs/zYBjGqDXfB4gMjKFnjQZkzKzLOth1D4VzACjLkgkoXvCNNb4tnoSBT2CcsuQGl8xY/uA7wd8v45C2bZmHbLdbZFmGuq6hqiosy4LneQysIQ+RnedBUVX1ywkzGT+IHZF5xhDvEKBt2zKFWpYlE398+KZnijPfB4h43ylc0guIjCN4ldlHluJ9PO+YpglN01DXNeI4xmq1QlEUMAwDhmHAsixpLiNqki5AeBOBFHObkwDpiipjuKXLg4g7FOVTQpemKaIoQtM0cByHJXYi8PxsD4HBe4UMzD4v6QWETOSNIW4RXV4ExHVdFEWBNE2x2Wxwd3cHy7Lw8uVLTCYTmKZ5tLxETxwy/hpa8pQKDNkoDunzCvH6oUMETtd1mKYJy7JgWRYURTniE1k7okrmTVwSMh55MiB9KrWPNPsAJCAMwwAAuK4L3/dh2zZs24aiKEiSBI7jHIXeLk8R3V/TNLRty4DhPY2uf/KS6ct0ZQOXmSzC8J7i+z6+/vpr5iV1XWO5XML3fbiui8lkAsdxjmZeFvlE8UXXUH1F7NOTAOG5oMv1x5AsdYCPWuTGs9kMP/zwA6uL/Pnnn/jjjz8QhiEmkwnevHkDx3GO+tU0DfMennD52edBk3nMkwDpWgJd52TXip7BX8dzwcPDA/b7PQ6HA8qyxH6/R57nOD8/x8uXL1HXNZqmQVEUqKoKQRAwzpENtIt3Pivs8gPkB9HlOUPFJFFsUe21rmt8+PABv/32G3zfh+d5iKIIZVni6uoKVVWhKAqUZYkoipDn+RHn8H0V+UEM1UOgjJLu4mD482MiCwku27aZzqjrmg2SMl0KxU3TsLbTNMV2u2WD2+/3SJIEFxcXvdxF1/PSXVxOTwJENvPAcarfp08otDqOg+l0ymanrmtkWYY8z1EUBfI8R57nqKoKcRwjCAJ4nseUrGma0HUdm80G+/0ei8VCOrAuku2S/icBIjbUFVFkoVVVVZa1mqYJAMiyjNU/0jRFHMfYbDb4+++/kWUZTNM8qosYhsEAyvMciqJgtVphs9kgiiKkacq2RHjjl4noHbJldRIgokeIn4HHWTHxA82qZVlomgZJkqCqKuR5jjRNkSQJVqsVbm9v2eDKskRRFFAUhQEURREb1MePH3F/f4/tdos0Tdly5MEQTQTjyWGXH7BIjvzBg6RpGhzHga7rsG2beQiFSlrTRVGgaRqW7dJBUSTPc2RZBgBHnkCc89dff0FVVVxfX7PtCr6PIjDU7hdZMjL9IeoQuo4AoayVVGlVVYwsFUVBlmUMmMPhgDRNkaYp6rpGXdfI85xtS6iqiiAIYNs2iqJAURT48OED4jjG5eUlZrNZZ/iVjeXJgAyFVd7IE3zfRxiGAMB0w3a7ZZ5A0YSWjaZpCMOQLam6rtkyoWp80zS4uLjAZDJB27YwTRNJkgD45D1VVbGSI/B42QyF5tGA8F7QBRJ/rW3bcF0Xs9mMDaooCtzf37M0n8CgCESAJEmCtm0ZIARekiTI85xV0Nq2hWEYjGip2kaapg8M2feTABEfJHqI+J0XWuTym80GHz9+xHa7xXK5RBRF2G63cF0XnufBsiy4rgvXdeE4DtMmwKdaa57nKMvyiGSpP0TY1K4svIrhd0i7jAZEfKBsOfGAFEWBOI6x2+2wWq2wXC7x/v17PDw8YLlcIgxDnJ2d4ZtvvsG3334L27aZFKdlAgBVVaGqKhbGKYQTN9HRJ7Z4gIZqrCdxiIiu6CkAWCJVFAV2ux02mw3++ecfrFYrrFYrrNdr3N3dYbfb4eHhAb7v4+3bt0eDr6qKJXv8Rju/vTGZTJiH8YMlo0RP9JTPDrt8ZBkCTATkcDhgv9/j4eEBm80Gm80G9/f3WK/X2O/32G63+O67747CMQk3Apl/x4Rvy3VdBEHAyJQ/z2e3vH122JURUp/HNE1zNDO0xn3fx+FwgOM4rDzoOA5evHjBxFiSJDgcDojjmG1YUQjnK2kUdqfTKebzOduyED2X+sP/NlQcGgVIF1+IXsPXIwgQymMoMyXlqiifEj56M4kiC5/bmKbJogdltfyWp+M4CMPwkXTn2+f7JysUPRmQLpPtefCzQIBQphsEAaIowmw2Y1pitVrh/fv3WK/XSJIEuq7jxYsXrEBEZAuARZy6ruF5HsIwhGmaj4hSXML0W9dSGg3IkMkySHGG+EKybdvwPA+e56FpGuR5jv1+j/V6zTQKeUQQBAiCgJEqH33atj3a4eMHLypWvj9fxEPor4xPZBmuuFZVVWVaYzqdIs9zxhX7/R5FUSCKIpimiaurK+YRVHgW6yW07IIgwHQ6Pdqy6Jsw8qKh8Dx6G0J2TiRWmauSPiEPcRwHnucxgUWaxXEcdlA4NU0TcRwDAAOFvIyEHM8foqfK+vvZVfeu4o8MpLqupW8CibKaRBUNbjqdIgxD9qIdlQvatkUcx6jrmkWcn376Ca9fv8bl5eXRlqdY9xBB+o+/HyKzpmnQNA0rCItrl/caAoU8h9cVhmEcSXjKknVdx6tXr7BYLBAEwZH24Mmyqzo2pmrWC4joXl0hmBogIMRnUEpfliVbx4ZhMI3BvydCGS4BQmWBd+/e4fLyEovFgu3j8G2TB3blMmMIdRAQHghZ7aOL0fmO0F/yHrqel+b8FgEtNZ5M67pGGIaYz+cIwxCu63b2Uxy87O2hL57LyDapyEi2k6qkv3VdQ1E+ZayTyYS5O68jKI9p2xZJkrCKme/7uLy8xNu3bxGGISNSfmAENl8moDHw4fiL65AuTuH3TmmJEMnSrJN6NU2TkS8/OLoHAJPoFG0mk4lUiHVtLYg7dTLvORmQrryF9xLqFMlxVVUfESsA9goVnadCclVVyLKM8QYBmOc56rrG9fU1FosFXr9+Dc/zHoXZLuKkdvha6hfRIX2cQZ/5cMrHeh4QihLiuyNUaKbraNkAgGmamM1mePXqFSaTyVF1fSil4EEZ6x2DgJB1lRHJSJ7rus7Sd3J/AoV/H5VP4kjC0/2GYUDXddzc3OD777/H9fU1U7CyEC6CIA6cNtaBcQneyRwicznxXzl4MHhtQpxCB/EFDxqp2fPzc1xdXbG3isQJEUEQz/Hnxy4XYKRSpYeK+zOapsGyLJydnaGqKrYRvdvtkGUZ24w6HA6Iogj39/fsO72bSrNG2uTm5gY///wz3rx5g/l8flQRo3b7wuuQIv1i9RDZb7zaJOO3Geig0Ev/+kH6gg+BmqbBtm1WZz07O4Pv+53ti4MTwZEtjTHy/aSNKt5DdF3H2dkZi/u6rmM6naJtWyyXS+x2O9ze3rKX+slLCBzR5vM53r17hx9//BGLxYJlteJgRYBENS3jCf6a/0g9hNcT/LokYqT3UMlbyCt4bhHNdV189dVXCIIAjuN05k1jvovFZdnnLnsSqVqWxcgP+FTJosam0ylubm5we3uL3W6Hpmlwd3eHLMukapEGfXFxgZubG5yfn/dm1DLjQ7+oO8ZmuWRP8hC+DCA2SLzCJ21dXsEbJXt0z+fYKbrj0b2nIvj/bs//yCzYMyCCPQMi2DMggj0DItgzIIL9C9b4/Uy+aVhUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = get_dataset(train_df)\n",
    "labels = get_labels(train_df)\n",
    "\n",
    "for image_tensor, label in list(dataset)[:2]:\n",
    "    print(label)\n",
    "    show_image(torch.reshape(image_tensor, (28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate random weights between -1 and 1 and mark them to calculate gradient by setting `.requires_grad_()` Autograd engine will track all the transformations/functions applied to weights in a graph structure (The leaves of this graph are input tensors and the roots are output tensors) and calculates the derivates when `.backward()` is called on the weights tensor.\n",
    "\n",
    "> Important: Know more about PyTorch Autograd [here](https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=25, bias=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_random_weights(rows, cols):\n",
    "    return torch.randn(rows, cols).to('cuda').requires_grad_()\n",
    "\n",
    "# pytorch equivalent\n",
    "parameters_py = nn.Linear(28*28, len(train_df.label.unique())+1)\n",
    "parameters_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 25]), torch.Size([1, 25]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each weight corresponds to each pixel of the input images\n",
    "weights = get_random_weights(28*28, len(train_df.label.unique())+1)\n",
    "bias = get_random_weights(1, len(train_df.label.unique())+1)\n",
    "parameters = (weights, bias)\n",
    "\n",
    "weights.shape, bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find predictions we need to multiply each weight with correponding pixel intensity, calculate the sum and add the bias this operation can be done without any loops by matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([21964, 784]), torch.Size([784, 25]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensors = get_image_tensors(train_df)\n",
    "image_tensors.shape, weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(image_tensors, weights, bias):\n",
    "    return image_tensors@weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21964, 25])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = get_predictions(image_tensors, weights, bias)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions with random weights are around ~5% accurate, lets try to optimise them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0406)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(df, parameters):\n",
    "    weights, bias = parameters\n",
    "    test_image_tensors, test_labels = get_image_tensors(valid_df), get_labels(valid_df)\n",
    "    test_preds = get_predictions(test_image_tensors, weights, bias)\n",
    "    pred_classes = torch.argmax(test_preds, dim=1)\n",
    "    return (pred_classes == test_labels).float().mean() \n",
    "\n",
    "accuracy(valid_df, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A loss function is a measure of how good your prediction model is in terms of being able to predict accurately. We turn the learning problem into an optimization problem by defining a loss function and optimise the parameters to minimise the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Softmax Function](images/softmax.png \"Softmax function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A softmax function turns predictions into probabilities, each value is bounded between (0, 1). Our model considers 0.900 and 0.999 as the same but the second prediction is 100 times more confident hence we use log to amplify the domain to (-inf, inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(predictions, log=False):\n",
    "    if log:\n",
    "        return torch.log(torch.exp(predictions)/torch.exp(predictions).sum(dim=1, keepdim=True))\n",
    "    return torch.exp(predictions)/torch.exp(predictions).sum(dim=1, keepdim=True)\n",
    "\n",
    "# pytorch equivalent\n",
    "activations_py = F.log_softmax(predictions, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.], grad_fn=<SelectBackward>),\n",
       " tensor([-inf], grad_fn=<SelectBackward>),\n",
       " torch.Size([21964, 1]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gathers the corresponsing loss according to the label class\n",
    "activations = softmax(predictions, log=True)\n",
    "loss_of_each_image = activations.gather(1, labels.unsqueeze(-1))\n",
    "max(loss_of_each_image), min(loss_of_each_image), loss_of_each_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe a problem here, If we are taking log after softmax we get loss as infinity to avoid that we use `F.log_softmax()` function provided by PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-6adc8555dfc7>:1: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.log(F.softmax(tensor([62.0, -51.0]), dim=0)), F.log_softmax(tensor([62.0, -51.0]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0., -inf]), tensor([   0., -113.]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(F.softmax(tensor([62.0, -51.0]), dim=0)), F.log_softmax(tensor([62.0, -51.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 25 probabilities corresponding to 25 categories for each image, we select the value according to it class it belongs and take the mean of all the training images to calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [5]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example:\n",
    "list_of_lists = tensor([[1, 2], [2, 5]])\n",
    "index = tensor([0, 1])\n",
    "list_of_lists.gather(1, index.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-40.5223, grad_fn=<MeanBackward0>),\n",
       " tensor(-40.5223, grad_fn=<NegBackward>))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gather will select values of each row of activations according to the index specified by labels\n",
    "loss = activations_py.gather(1, labels.unsqueeze(-1)).mean()\n",
    "\n",
    "# pytorch equivalent \n",
    "loss_py = -F.nll_loss(activations_py, labels)\n",
    "\n",
    "loss, loss_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(40.5223, grad_fn=<NegBackward>),\n",
       " tensor(40.5223, grad_fn=<NllLossBackward>))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax + log + negative_loss_likelyhood = cross_entropy_loss: this can be used for classification problems\n",
    "def cross_entropy_loss(predictions, labels): \n",
    "    activations = F.log_softmax(predictions, dim=1)\n",
    "    loss = activations.gather(1, labels.unsqueeze(-1)).mean()\n",
    "    return -loss\n",
    "\n",
    "# pytorch equivalent\n",
    "cross_entropy_loss_py = F.cross_entropy(predictions, labels)\n",
    "\n",
    "cross_entropy_loss(predictions, labels), cross_entropy_loss_py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimise weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, parameters, learning_rate, accuracy_func, loss_func, valid_df, epoch_no):\n",
    "    for image_tensors, labels in dataloader:\n",
    "        weights, bias = parameters\n",
    "        preds = get_predictions(image_tensors, weights, bias)\n",
    "        \n",
    "        loss = cross_entropy_loss(preds, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        weights.data = weights.data - weights.grad*learning_rate\n",
    "        bias.data -= bias.grad*learning_rate\n",
    "        weights.grad.zero_()\n",
    "        bias.grad.zero_()\n",
    "        \n",
    "    accuracy = accuracy_func(valid_df, (weights, bias))\n",
    "    if epoch_no%5 == 0:\n",
    "        print(f'epoch={epoch_no}; loss={loss}; accuracy={accuracy}')\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(parameters, no_of_epochs, learning_rate, batch_size):\n",
    "    for i in range(1, no_of_epochs+1):\n",
    "        dl = DataLoader(get_dataset(train_df), batch_size=batch_size)\n",
    "        weights, bias = train_epoch(dl, parameters, learning_rate, accuracy, cross_entropy_loss, valid_df, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5; loss=7.723250865936279; accuracy=0.05882352963089943\n",
      "epoch=10; loss=6.59684419631958; accuracy=0.07885631173849106\n",
      "epoch=15; loss=5.713827133178711; accuracy=0.10872336477041245\n",
      "epoch=20; loss=5.041042327880859; accuracy=0.13713349401950836\n",
      "epoch=25; loss=4.521183967590332; accuracy=0.1637224555015564\n",
      "epoch=30; loss=4.11172342300415; accuracy=0.18575851619243622\n",
      "epoch=35; loss=3.7844295501708984; accuracy=0.20961573719978333\n",
      "epoch=40; loss=3.5177741050720215; accuracy=0.23383718729019165\n",
      "epoch=45; loss=3.2960424423217773; accuracy=0.2586049735546112\n",
      "epoch=50; loss=3.108018636703491; accuracy=0.28337278962135315\n"
     ]
    }
   ],
   "source": [
    "train(parameters, no_of_epochs=50, learning_rate=1e-1, batch_size=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started with random parameters whos predictions are ~4% after training for 50 epochs I was able to achieve 28% accuracy, lets train for some more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5; loss=2.7389485836029053; accuracy=0.3208887279033661\n",
      "epoch=10; loss=2.5074076652526855; accuracy=0.34984520077705383\n",
      "epoch=15; loss=2.3198177814483643; accuracy=0.38025858998298645\n",
      "epoch=20; loss=2.163520574569702; accuracy=0.4079402685165405\n",
      "epoch=25; loss=2.0304787158966064; accuracy=0.43398287892341614\n",
      "epoch=30; loss=1.9153084754943848; accuracy=0.45784008502960205\n",
      "epoch=35; loss=1.8142876625061035; accuracy=0.47532325983047485\n",
      "epoch=40; loss=1.7247360944747925; accuracy=0.49280640482902527\n",
      "epoch=45; loss=1.6446409225463867; accuracy=0.5070114731788635\n",
      "epoch=50; loss=1.5724561214447021; accuracy=0.520852267742157\n"
     ]
    }
   ],
   "source": [
    "train(parameters, no_of_epochs=50, learning_rate=1e-1, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm trying to play with batch_size and learning rates to see how they effect the traning speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5; loss=1.4595612287521362; accuracy=0.5585503578186035\n",
      "epoch=10; loss=1.2876158952713013; accuracy=0.6040793657302856\n",
      "epoch=15; loss=1.157658576965332; accuracy=0.6421416997909546\n",
      "epoch=20; loss=1.0544577836990356; accuracy=0.6709160208702087\n",
      "epoch=25; loss=0.9689654111862183; accuracy=0.6958659291267395\n",
      "epoch=30; loss=0.8959687352180481; accuracy=0.7175377607345581\n",
      "epoch=35; loss=0.8325452208518982; accuracy=0.741030752658844\n",
      "epoch=40; loss=0.7769091725349426; accuracy=0.7576033473014832\n",
      "epoch=45; loss=0.7277417778968811; accuracy=0.7723547220230103\n",
      "epoch=50; loss=0.6839793920516968; accuracy=0.788563072681427\n"
     ]
    }
   ],
   "source": [
    "train(parameters, no_of_epochs=50, learning_rate=2e-1, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bumping up the learning rate indeed improved the optimisation speed, let train for some more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5; loss=0.6447545886039734; accuracy=0.8027681708335876\n",
      "epoch=10; loss=0.6093621253967285; accuracy=0.8149699568748474\n",
      "epoch=15; loss=0.577233076095581; accuracy=0.8229830265045166\n",
      "epoch=20; loss=0.5479092597961426; accuracy=0.8319067358970642\n",
      "epoch=25; loss=0.5210235714912415; accuracy=0.8402841091156006\n",
      "epoch=30; loss=0.49628016352653503; accuracy=0.8466581702232361\n",
      "epoch=35; loss=0.4734363555908203; accuracy=0.8553997278213501\n",
      "epoch=40; loss=0.45229026675224304; accuracy=0.8639591932296753\n",
      "epoch=45; loss=0.4326697885990143; accuracy=0.8721544146537781\n",
      "epoch=50; loss=0.4144253730773926; accuracy=0.880167543888092\n"
     ]
    }
   ],
   "source": [
    "train(parameters, no_of_epochs=50, learning_rate=2e-1, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try with a larger batch size and learning rate, maybe it generalises well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5; loss=0.45226046442985535; accuracy=0.8226187825202942\n",
      "epoch=10; loss=0.41609740257263184; accuracy=0.8228009343147278\n",
      "epoch=15; loss=0.4397861361503601; accuracy=0.8209797739982605\n",
      "epoch=20; loss=0.4218664765357971; accuracy=0.827353835105896\n",
      "epoch=25; loss=0.42012253403663635; accuracy=0.8288107514381409\n",
      "epoch=30; loss=0.40878626704216003; accuracy=0.8324530720710754\n",
      "epoch=35; loss=0.3948947787284851; accuracy=0.8741576671600342\n",
      "epoch=40; loss=0.39441171288490295; accuracy=0.8421052694320679\n",
      "epoch=45; loss=0.3591289520263672; accuracy=0.8845382928848267\n",
      "epoch=50; loss=0.3269328773021698; accuracy=0.8943725824356079\n"
     ]
    }
   ],
   "source": [
    "train(parameters, no_of_epochs=50, learning_rate=3e-1, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5; loss=0.2853115499019623; accuracy=0.9286104440689087\n",
      "epoch=10; loss=0.2771397829055786; accuracy=0.9318885207176208\n",
      "epoch=15; loss=0.26912689208984375; accuracy=0.9358950853347778\n",
      "epoch=20; loss=0.26137667894363403; accuracy=0.93917316198349\n",
      "epoch=25; loss=0.25392618775367737; accuracy=0.9429976344108582\n",
      "epoch=30; loss=0.2467852532863617; accuracy=0.9462757110595703\n",
      "epoch=35; loss=0.23995234072208405; accuracy=0.9501001238822937\n",
      "epoch=40; loss=0.2334199845790863; accuracy=0.9513749480247498\n",
      "epoch=45; loss=0.227177232503891; accuracy=0.9535603523254395\n",
      "epoch=50; loss=0.22121219336986542; accuracy=0.957202672958374\n"
     ]
    }
   ],
   "source": [
    "train(parameters, no_of_epochs=50, learning_rate=2e-1, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are now a bit above 95% accuracy, let try to take smaller steps to prevent overshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5; loss=0.21588782966136932; accuracy=0.9579311609268188\n",
      "epoch=10; loss=0.21408985555171967; accuracy=0.9582953453063965\n",
      "epoch=15; loss=0.21236322820186615; accuracy=0.9584774971008301\n",
      "epoch=20; loss=0.21069581806659698; accuracy=0.9588417410850525\n",
      "epoch=25; loss=0.20907911658287048; accuracy=0.9595701694488525\n",
      "epoch=30; loss=0.2075067162513733; accuracy=0.9595701694488525\n",
      "epoch=35; loss=0.20597346127033234; accuracy=0.9601165056228638\n",
      "epoch=40; loss=0.20447543263435364; accuracy=0.9610270857810974\n",
      "epoch=45; loss=0.20300959050655365; accuracy=0.9615734815597534\n",
      "epoch=50; loss=0.20157337188720703; accuracy=0.9623019099235535\n"
     ]
    }
   ],
   "source": [
    "train(parameters, no_of_epochs=50, learning_rate=1e-1, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we get more out the single layer NN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5; loss=0.20638421177864075; accuracy=0.9641230702400208\n",
      "epoch=10; loss=0.20571450889110565; accuracy=0.9641230702400208\n",
      "epoch=15; loss=0.20504949986934662; accuracy=0.9641230702400208\n",
      "epoch=20; loss=0.2043890506029129; accuracy=0.9646694660186768\n",
      "epoch=25; loss=0.20373308658599854; accuracy=0.9650336503982544\n",
      "epoch=30; loss=0.20308151841163635; accuracy=0.9653978943824768\n",
      "epoch=35; loss=0.20243410766124725; accuracy=0.9655800461769104\n",
      "epoch=40; loss=0.20179085433483124; accuracy=0.9655800461769104\n",
      "epoch=45; loss=0.2011517584323883; accuracy=0.965944230556488\n",
      "epoch=50; loss=0.2005166858434677; accuracy=0.965944230556488\n"
     ]
    }
   ],
   "source": [
    "train(parameters, no_of_epochs=50, learning_rate=1e-1, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5; loss=0.21264278888702393; accuracy=0.9639409780502319\n",
      "epoch=10; loss=0.20781667530536652; accuracy=0.9661263823509216\n",
      "epoch=15; loss=0.2031080722808838; accuracy=0.9674012064933777\n",
      "epoch=20; loss=0.1985561102628708; accuracy=0.9688581228256226\n",
      "epoch=25; loss=0.19416968524456024; accuracy=0.9703150391578674\n",
      "epoch=30; loss=0.1899484097957611; accuracy=0.9723182916641235\n",
      "epoch=35; loss=0.1858879029750824; accuracy=0.9750500321388245\n",
      "epoch=40; loss=0.18198280036449432; accuracy=0.9768711924552917\n",
      "epoch=45; loss=0.17822673916816711; accuracy=0.9781460165977478\n",
      "epoch=50; loss=0.17461314797401428; accuracy=0.9797850847244263\n"
     ]
    }
   ],
   "source": [
    "train(parameters, no_of_epochs=50, learning_rate=1e-1, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to me that having lower batch size is better? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5; loss=0.1711360067129135; accuracy=0.9810599088668823\n",
      "epoch=10; loss=0.16778911650180817; accuracy=0.9825168251991272\n",
      "epoch=15; loss=0.1645662933588028; accuracy=0.9836094975471497\n",
      "epoch=20; loss=0.1614619493484497; accuracy=0.9845200777053833\n",
      "epoch=25; loss=0.15847033262252808; accuracy=0.9852485656738281\n",
      "epoch=30; loss=0.1555863618850708; accuracy=0.9865233898162842\n",
      "epoch=35; loss=0.1528049111366272; accuracy=0.9872518181800842\n",
      "epoch=40; loss=0.15012092888355255; accuracy=0.9874339699745178\n",
      "epoch=45; loss=0.14753028750419617; accuracy=0.9885266423225403\n",
      "epoch=50; loss=0.14502808451652527; accuracy=0.9892551302909851\n"
     ]
    }
   ],
   "source": [
    "train(parameters, no_of_epochs=50, learning_rate=1e-1, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I never assumed we could get 98.9% accurate model with just 1 layer, lets test the accuracy with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9893)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(test_df, (weights, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5; loss=0.14261044561862946; accuracy=0.9901657104492188\n",
      "epoch=10; loss=0.14027352631092072; accuracy=0.9905299544334412\n",
      "epoch=15; loss=0.13801345229148865; accuracy=0.9908941388130188\n",
      "epoch=20; loss=0.1358269453048706; accuracy=0.9910762906074524\n",
      "epoch=25; loss=0.13371039927005768; accuracy=0.9912583827972412\n",
      "epoch=30; loss=0.13166065514087677; accuracy=0.9914405345916748\n",
      "epoch=35; loss=0.12967489659786224; accuracy=0.9923511147499084\n",
      "epoch=40; loss=0.12775032222270966; accuracy=0.9925332069396973\n",
      "epoch=45; loss=0.12588419020175934; accuracy=0.9928974509239197\n",
      "epoch=50; loss=0.12407400459051132; accuracy=0.9938080310821533\n"
     ]
    }
   ],
   "source": [
    "train(parameters, no_of_epochs=50, learning_rate=1e-1, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5; loss=0.11372235417366028; accuracy=0.9952649474143982\n",
      "epoch=10; loss=0.11295342445373535; accuracy=0.9954470992088318\n",
      "epoch=15; loss=0.11219793558120728; accuracy=0.9956291913986206\n",
      "epoch=20; loss=0.11145418137311935; accuracy=0.9958112835884094\n",
      "epoch=25; loss=0.11072102189064026; accuracy=0.995993435382843\n",
      "epoch=30; loss=0.10999791324138641; accuracy=0.995993435382843\n",
      "epoch=35; loss=0.10928422957658768; accuracy=0.995993435382843\n",
      "epoch=40; loss=0.10857987403869629; accuracy=0.9961755275726318\n",
      "epoch=45; loss=0.10788467526435852; accuracy=0.9961755275726318\n",
      "epoch=50; loss=0.10719814896583557; accuracy=0.9963576793670654\n"
     ]
    }
   ],
   "source": [
    "train(parameters, no_of_epochs=50, learning_rate=1e-1, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9964)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(test_df, (weights, bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yea! its around 99.6% accurate. I still tried to train for more epochs but seemed to overfit (loss imporves but the accuracy decreases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
