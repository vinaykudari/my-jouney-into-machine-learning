{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"American sign language classifier from scratch\"\n",
    "\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [classifier, scratch, vision]\n",
    "- hide: false\n",
    "- search_exclude: true\n",
    "- metadata_key1: metadata_value1\n",
    "- metadata_key2: metadata_value2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import pandas as pd\n",
    "from os import path\n",
    "# torch.cuda.set_device(0)\n",
    "# torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21964, 785), (5491, 785), (7172, 785))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if path.exists('/storage'):\n",
    "#     # Paperspace\n",
    "#     train_df = pd.read_csv('/storage/data/asl/sign_mnist_train.csv')\n",
    "#     test_df = pd.read_csv('/storage/data/asl/sign_mnist_test.csv')\n",
    "# else:\n",
    "#     # GCP\n",
    "#     train_df = pd.read_csv('/home/jupyter/datasets/asl/sign_mnist_train.csv')\n",
    "#     test_df = pd.read_csv('/home/jupyter/datasets/asl/sign_mnist_test.csv') \n",
    "\n",
    "# Randomly select 20% of training data as validation data\n",
    "valid_df = train_df.sample(frac=0.2)\n",
    "train_df = train_df.drop(valid_df.index)\n",
    "\n",
    "train_df.shape, valid_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>135</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      3     107     118     127     134     139     143     146     150   \n",
       "1      6     155     157     156     156     156     157     156     158   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     153  ...       207       207       207       207       206       206   \n",
       "1     158  ...        69       149       128        87        94       163   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       206       204       203       202  \n",
       "1       175       103       135       149  \n",
       "\n",
       "[2 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our training images in a dataframe where 1st value is the label and next 784 values are pixel intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all the labels\n",
    "def get_labels(df):\n",
    "    return tensor(df.iloc[:, 0].values).to('cuda')\n",
    "\n",
    "# Select all the pixel intensities and convert into a 1-D tensor of values between 0 to 1\n",
    "def get_image_tensors(df):\n",
    "    return torch.stack([tensor(image_array)/255. for image_array in df.iloc[:, 1:].values]).to('cuda')\n",
    "\n",
    "def get_dataset(df):\n",
    "    return zip(get_image_tensors(df), get_labels(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3, device='cuda:0')\n",
      "tensor(6, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = get_dataset(train_df)\n",
    "labels = get_labels(train_df)\n",
    "\n",
    "for image_tensor, label in list(dataset)[:2]:\n",
    "    print(label)\n",
    "    show_image(torch.reshape(image_tensor, (28, 28)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random weights between -1 and 1 and mark them to calculate gradient\n",
    "def get_random_weights(rows, cols):\n",
    "    return torch.randn(rows, cols).to('cuda').requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 25]), torch.Size([1, 25]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = get_random_weights(28*28, len(train_df.label.unique())+1)\n",
    "bias = get_random_weights(1, len(train_df.label.unique())+1)\n",
    "parameters = (weights, bias)\n",
    "\n",
    "weights.shape, bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([21964, 784]), torch.Size([784, 25]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensors = get_image_tensors(train_df)\n",
    "image_tensors.shape, weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(image_tensors, weights, bias):\n",
    "    return image_tensors@weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21964, 25])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = get_predictions(image_tensors, weights, bias)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy with random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0419, device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(df, parameters):\n",
    "    weights, bias = parameters\n",
    "    test_image_tensors, test_labels = get_image_tensors(valid_df), get_labels(valid_df)\n",
    "    test_preds = get_predictions(test_image_tensors, weights, bias)\n",
    "    pred_classes = torch.argmax(test_preds, dim=1)\n",
    "    return (pred_classes == test_labels).float().mean() \n",
    "\n",
    "accuracy(valid_df, parameters)\n",
    "# Random weights are ~5% accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As softmax function turns predictions into probabilities, each activation is bounded between (0, 1) hence our model considers 0.900 and 0.999 as the same but the second prediction is 100 times more confident hence we use log to amplify the domain to (-inf, inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(10.7700, device='cuda:0', grad_fn=<SelectBackward>),\n",
       " tensor(1.6697e-11, device='cuda:0', grad_fn=<SelectBackward>),\n",
       " tensor(-24.8158, device='cuda:0', grad_fn=<SelectBackward>),\n",
       " tensor(-24.8158, device='cuda:0', grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(predictions, log=False):\n",
    "    if log:\n",
    "        return torch.log(torch.exp(predictions)/torch.exp(predictions).sum(dim=1, keepdim=True))\n",
    "    return torch.exp(predictions)/torch.exp(predictions).sum(dim=1, keepdim=True)\n",
    "\n",
    "# pytorch equivalent\n",
    "activations_py = F.log_softmax(predictions, dim=1)\n",
    "\n",
    "predictions[0][0], softmax(predictions)[0][0], softmax(predictions, log=True)[0][0], activations_py[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.], device='cuda:0', grad_fn=<SelectBackward>),\n",
       " tensor([-inf], device='cuda:0', grad_fn=<SelectBackward>),\n",
       " torch.Size([21964, 1]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gathers the corresponsing loss according to the label class\n",
    "loss_of_each_image = activations.gather(1, labels.unsqueeze(-1))\n",
    "max(loss_of_each_image), min(loss_of_each_image), loss_of_each_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a problem with applying log after softmax function, hence we use F.log_softmax() function provided by PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-47-6adc8555dfc7>:1: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.log(F.softmax(tensor([62.0, -51.0]), dim=0)), F.log_softmax(tensor([62.0, -51.0]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0., -inf]), tensor([   0., -113.]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(F.softmax(tensor([62.0, -51.0]), dim=0)), F.log_softmax(tensor([62.0, -51.0]))a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-37.4764, device='cuda:0', grad_fn=<MeanBackward0>),\n",
       " tensor(-37.4764, device='cuda:0', grad_fn=<NegBackward>))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = activations_py.gather(1, labels.unsqueeze(-1)).mean()\n",
    "\n",
    "# pytorch equivalent \n",
    "loss_py = -F.nll_loss(activations_py, labels)\n",
    "\n",
    "loss, loss_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(37.4764, device='cuda:0', grad_fn=<NegBackward>),\n",
       " tensor(37.4764, device='cuda:0', grad_fn=<NllLossBackward>))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax + log + negative_loss_likelyhood = cross_entropy_loss: this can be used for classification problems\n",
    "def cross_entropy_loss(predictions, labels): \n",
    "    activations = F.log_softmax(predictions, dim=1)\n",
    "    loss = activations.gather(1, labels.unsqueeze(-1)).mean()\n",
    "    return -loss\n",
    "\n",
    "# pytorch equivalent\n",
    "cross_entropy_loss_py = F.cross_entropy(predictions, labels)\n",
    "\n",
    "cross_entropy_loss(predictions, labels), cross_entropy_loss_py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimise weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, parameters, learning_rate, accuracy_func, valid_df, epoch_no):\n",
    "    for image_tensors, labels in dataloader:\n",
    "        weights, bias = parameters\n",
    "        preds = get_predictions(image_tensors, weights, bias)\n",
    "        \n",
    "        loss = cross_entropy_loss(preds, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        weights.data = weights.data - weights.grad*learning_rate\n",
    "        bias.data -= bias.grad*learning_rate\n",
    "        weights.grad.zero_()\n",
    "        bias.grad.zero_()\n",
    "        \n",
    "    accuracy = accuracy_func(valid_df, (weights, bias))\n",
    "    if epoch_no%5 == 0:\n",
    "        print(f'epoch={epoch_no}; loss={loss}; accuracy={accuracy}')\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(parameters, no_of_epochs, learning_rate, batch_size):\n",
    "    for i in range(1, no_of_epochs+1):\n",
    "        dl = DataLoader(get_dataset(train_df), batch_size=batch_size)\n",
    "        weights, bias = train_epoch(dl, parameters, learning_rate, accuracy, valid_df, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5; loss=2.197233200073242; accuracy=0.3982881009578705\n",
      "epoch=10; loss=2.051445722579956; accuracy=0.4283372759819031\n",
      "epoch=15; loss=1.9267258644104004; accuracy=0.45346930623054504\n",
      "epoch=20; loss=1.8183813095092773; accuracy=0.4744126498699188\n",
      "epoch=25; loss=1.7231673002243042; accuracy=0.4929885268211365\n",
      "epoch=30; loss=1.6387637853622437; accuracy=0.5155709385871887\n",
      "epoch=35; loss=1.5634348392486572; accuracy=0.5310508012771606\n",
      "epoch=40; loss=1.4958091974258423; accuracy=0.5439810752868652\n",
      "epoch=45; loss=1.4347560405731201; accuracy=0.5543616414070129\n",
      "epoch=50; loss=1.3793251514434814; accuracy=0.5654707551002502\n"
     ]
    }
   ],
   "source": [
    "train(parameters, no_of_epochs=50, learning_rate=1e-1, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5; loss=1.332802176475525; accuracy=0.5784010291099548\n",
      "epoch=10; loss=1.3075464963912964; accuracy=0.5838645100593567\n",
      "epoch=15; loss=1.2833633422851562; accuracy=0.5895100831985474\n",
      "epoch=20; loss=1.260180950164795; accuracy=0.5953378081321716\n",
      "epoch=25; loss=1.2379337549209595; accuracy=0.6015297770500183\n",
      "epoch=30; loss=1.2165626287460327; accuracy=0.6057184338569641\n",
      "epoch=35; loss=1.1960129737854004; accuracy=0.6106355786323547\n",
      "epoch=40; loss=1.1762347221374512; accuracy=0.6184665560722351\n",
      "epoch=45; loss=1.1571820974349976; accuracy=0.6224731206893921\n",
      "epoch=50; loss=1.1388126611709595; accuracy=0.6277545094490051\n"
     ]
    }
   ],
   "source": [
    "train(parameters, no_of_epochs=50, learning_rate=1e-1, batch_size=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the learning seems to be slow we can increase the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5; loss=1.210028886795044; accuracy=0.6099070906639099\n",
      "epoch=10; loss=1.389617681503296; accuracy=0.6073575019836426\n",
      "epoch=15; loss=1.1752502918243408; accuracy=0.6150063872337341\n",
      "epoch=20; loss=1.1438595056533813; accuracy=0.6357675790786743\n",
      "epoch=25; loss=1.0695892572402954; accuracy=0.6654525399208069\n",
      "epoch=30; loss=1.1350699663162231; accuracy=0.6567109823226929\n",
      "epoch=35; loss=1.1996654272079468; accuracy=0.6572573184967041\n",
      "epoch=40; loss=1.0542986392974854; accuracy=0.6873064637184143\n",
      "epoch=45; loss=0.891987144947052; accuracy=0.7131670117378235\n",
      "epoch=50; loss=0.9970915913581848; accuracy=0.689491868019104\n"
     ]
    }
   ],
   "source": [
    "train(parameters, no_of_epochs=50, learning_rate=3e-1, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5; loss=0.5388020277023315; accuracy=0.8198870420455933\n",
      "epoch=10; loss=0.4986553192138672; accuracy=0.8228009343147278\n",
      "epoch=15; loss=0.4944797158241272; accuracy=0.8269895911216736\n",
      "epoch=20; loss=0.56035977602005; accuracy=0.7914769649505615\n",
      "epoch=25; loss=0.524692714214325; accuracy=0.801311194896698\n",
      "epoch=30; loss=0.40703633427619934; accuracy=0.8324530720710754\n",
      "epoch=35; loss=0.44167599081993103; accuracy=0.819340705871582\n",
      "epoch=40; loss=0.41297367215156555; accuracy=0.8442906141281128\n",
      "epoch=45; loss=0.4123932123184204; accuracy=0.8535785675048828\n",
      "epoch=50; loss=0.4013936519622803; accuracy=0.8594062924385071\n"
     ]
    }
   ],
   "source": [
    "train(parameters, no_of_epochs=50, learning_rate=4e-1, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5; loss=0.3804209530353546; accuracy=0.8603168725967407\n",
      "epoch=10; loss=0.30301526188850403; accuracy=0.8767073154449463\n",
      "epoch=15; loss=0.34746670722961426; accuracy=0.8706974983215332\n",
      "epoch=20; loss=0.3037199378013611; accuracy=0.9082134366035461\n",
      "epoch=25; loss=0.26886311173439026; accuracy=0.8921871781349182\n",
      "epoch=30; loss=0.2956759035587311; accuracy=0.8892733454704285\n",
      "epoch=35; loss=0.3053615987300873; accuracy=0.8728829026222229\n",
      "epoch=40; loss=0.3092946410179138; accuracy=0.870879590511322\n",
      "epoch=45; loss=0.2900395691394806; accuracy=0.880167543888092\n",
      "epoch=50; loss=0.2876986861228943; accuracy=0.8807138800621033\n"
     ]
    }
   ],
   "source": [
    "train(parameters, no_of_epochs=50, learning_rate=4e-1, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5; loss=0.18215548992156982; accuracy=0.9544709324836731\n",
      "epoch=10; loss=0.18170930445194244; accuracy=0.9551994204521179\n",
      "epoch=15; loss=0.18126404285430908; accuracy=0.9555636048316956\n",
      "epoch=20; loss=0.18082618713378906; accuracy=0.9555636048316956\n",
      "epoch=25; loss=0.18039508163928986; accuracy=0.955927848815918\n",
      "epoch=30; loss=0.17997007071971893; accuracy=0.9562920928001404\n",
      "epoch=35; loss=0.17955094575881958; accuracy=0.9566563367843628\n",
      "epoch=40; loss=0.17913728952407837; accuracy=0.957202672958374\n",
      "epoch=45; loss=0.1787288933992386; accuracy=0.9575669169425964\n",
      "epoch=50; loss=0.17832538485527039; accuracy=0.9575669169425964\n"
     ]
    }
   ],
   "source": [
    "train(parameters, no_of_epochs=50, learning_rate=1e-1, batch_size=4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to achieve ~96% accuracy with just 1 layer of parameters"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
